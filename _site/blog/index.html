<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lasse Arpe Kristensen</title>

<meta name="description" content="Lasse Arpe Kristensen is a computer science student in Denmark." />

<meta name="keywords" content="computer science, machine learning">
<meta name="author" content="Lasse Arpe Kristensen">
<link rel="stylesheet" href="../assets/blog.css"/>
<!-- <link rel="shortcut icon" href="/favicon.png?v=e"> -->
</head>

<body>
    <div class="nav">
        <ul class="wrap">
            <li><a href="../">Home</a></li>
        </ul>
    </div>
    <div id='blog' class='wrap'>
        <div id='posts' class='section'>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2025/10/06/Markov-Inequality-intuition/">
                            Intuition of the Markov Inequality
                        </a>
                    </p>
                    <p class='post-date'>
                        06 October 2025
                    </p>
                </div>
                <p class='post-subtitle'>
                    In short, we have to keep the expectation intact.
                </p>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2025/10/06/Regularization/">
                            Inductive Bias and the Weight Competition
                        </a>
                    </p>
                    <p class='post-date'>
                        06 October 2025
                    </p>
                </div>
                <p class='post-subtitle'>
                    In essence, regularization can be viewed as a competition between weights.
                </p>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2025/09/20/Probability-and-Likelihood/">
                            Probability and Likelihood
                        </a>
                    </p>
                    <p class='post-date'>
                        20 September 2025
                    </p>
                </div>
                <p class='post-subtitle'>
                    Probability vs likelihood is an important distinction within statistics and probability theory. I found it confusing myself, so i set out to explain it.
                </p>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2025/04/04/KMP/">
                            Amortizing the Knuth Morris Pratt Algorithm
                        </a>
                    </p>
                    <p class='post-date'>
                        04 April 2025
                    </p>
                </div>
                <p class='post-subtitle'>
                    Where I try to simply explain *why* the KMP algorithm have an amortized linear runtime.
                </p>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2025/01/31/What/">
                            Simple explanation of the calculations inside of a neural network.
                        </a>
                    </p>
                    <p class='post-date'>
                        31 January 2025
                    </p>
                </div>
                <p class='post-subtitle'>
                    What actually happens *inside* of a neural network training session? A (very) simple, purely theoretical, example of the calculations happening in one training session inside of a neural network.
                </p>
            
        </div>
    </div>
</body>

</html>

