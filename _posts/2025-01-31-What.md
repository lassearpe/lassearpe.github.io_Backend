---
title: Simple explanation of the calculations inside of a neural network.
subtitle: What actually happens *inside* of a neural network training session? A (very) simple, purely theoretical, example of the calculations happening in one training session inside of a neural network. 
layout: default
date: 2025-01-31
keywords: deep-learning, neural-networks, math
published: true
---

In the spirit of 3blue1brown, neural networks can be explained visually. This certainly helps me in my understanding; for my own sake, I decided to write down a simple example of a single neural network training phase.
I will try to keep most of the related machine learning topics to a minimum, and focus mostly on the actual calculation. 

## Introduction to the structure of a neural network 
Despite this being a simple explanation, I want to get some really basic prerequisites out of the way. A neural network consist of neurons and layers, each layer consisting of several neurons, mimicing neurons inside of the biological brain. 

The first layer is an *input layer*, the middle layers are *hidden layers* and the terminal layer is the *output layer*.

The *node* concept is essential in the calculative part of the neural network.  
It is related to the basic [function](https://en.wikipedia.org/wiki/Function_(mathematics)) in mathmatics, mapping an input to an output. 
{% katex display %}
y=a+bx$
{% endkatex  %}

Only, in deep learning, we utilize *weights* (a,b) for each of our *features* (x). 

Features correspond to the variables associated with the prediction of interest, and our network is tasked with finding the weight minimizing the difference between our prediction and the observed output value. 

Because we have multiple features, we represent our neurons as matrix calculations. 

That is, a neural network is nothing more than a series of matrix multiplications. 

{% katexmm %}
Then $h(x)=w_0+w_1x$ becomes $h(x)=W_0+W_1^Tx$ as defined by the properties of matrix multiplications.
{% endkatexmm %}

In machine learning jargon, this translates to:

{% katex %}
z=w^T_j\sigma(z)+b_j
{% endkatex %}

Let me provide an example.

Say we have a neural network that looks roughly like this. 

*pic of neural network*

{% katex display %}
\begin{bmatrix} z_{1}  \\ \vdots \\ z_{n}\end{bmatrix}= \begin{bmatrix} w_{0,0} & w_{0,1} \\ w_{1,0} &w_{1,1} \\ \vdots & \vdots \\ w_{n,0} & w_{n,1}\end{bmatrix} \begin{bmatrix}  x_{1}& ... & x_n  \\    x_{n1}  & ... &x_{n,m}\end{bmatrix}+\begin{bmatrix} b_{0} \\   b_1 \end{bmatrix}
{% endkatex %}

Dimensionality is of great importance when regarding matrix multiplication.

{% katexmm %}
p \times m
{% endkatexmm %}

*The shifting of the dimensions of each neuron in the following layers*

**For hvert lag i netværket vil der være en vægtmatrice**, som repræsenterer forandringen i vores input på tværs af dette lag til et nyt. 

**Antallet af rækker i vægtmatricen** svarer til antallet af neuroner i det næste lag. 

**Antallet af kolonner i vægtmatricen** svarer til antallet af input features / neuroner i det nuværende lag. 


## Introduction to learning in a neural network

## Example of a single training session

